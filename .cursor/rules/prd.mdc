---
description: 
globs: 
alwaysApply: false
---
# ğŸ“„ AI Voice Assistant â€” Product Requirements Document (PRD)

## ğŸ§­ Goal

Build a SwiftUI-based iOS app with a minimalistic UI that enables users to interact with ElevenLabsâ€™ conversational AI using their voice. The interaction starts/stops via a single button.

---

## ğŸ¯ Core Features

1. **Single Call Button**

   - Green with phone icon by default
   - Turns red when active (i.e., streaming voice)
   - Tap toggles call state

2. **Microphone Permission**

   - Ask for `AVAudioSession` and `AVCaptureDevice` permissions on first use
   - Show user-friendly error if denied

3. **Voice Streaming with ElevenLabs**

   - Stream live voice to 11labsâ€™ Conversational API
   - Receive AI-generated voice response
   - Play AI response using `AVPlayer`

4. **Graceful Connect/Disconnect**

   - On connect: prepare audio session, initialize stream
   - On disconnect: terminate connection, clean up resources

5. **Minimal UI**

   - SwiftUI interface only
   - No navigation, no other screens
   - Responsive to device orientation and safe area

---

## ğŸ§± Project Structure

```
AIVoiceAssistant/
â”‚
â”œâ”€â”€ AIVoiceAssistantApp.swift         # Entry point
â”œâ”€â”€ ContentView.swift                 # UI with call button
â”‚
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ MicPermissionService.swift    # Request/check mic access
â”‚   â”œâ”€â”€ ElevenLabsService.swift       # Handles API streaming logic
â”‚   â””â”€â”€ AudioPlaybackService.swift    # Plays AI voice output
â”‚
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ CallState.swift               # Enum: .disconnected, .connecting, .connected
â”‚
â”œâ”€â”€ Utils/
â”‚   â””â”€â”€ AudioUtils.swift              # Helpers for audio session setup
â”‚
â”œâ”€â”€ Assets.xcassets/                 # App icon, colors, etc.
â””â”€â”€ Info.plist                       # Permissions, networking capabilities
```

---

## ğŸ” Permissions

- `NSMicrophoneUsageDescription`: "We need access to your microphone for live AI voice interaction."

---

## ğŸ“¡ API Integration (ElevenLabs Conversational API)

- Auth: API Key via `Authorization: Bearer`
- Input: Streaming PCM audio (or appropriate encoding)
- Output: Streaming audio (e.g., Opus, MP3) or JSON chunks
- Handle reconnection, errors, and timeout gracefully
- Provide silence detection or manual stop

---

## ğŸ”„ State Management

Use `@State` and `@ObservedObject` for:

- Current `CallState`
- Button color/icon toggle
- Audio input/output state

---

## ğŸ“± UI Interaction Flow

| Action       | Visual Change | Behind the Scenes                       |
| ------------ | ------------- | --------------------------------------- |
| App Launch   | Green button  | Wait for mic permission                 |
| Button Press | Red button    | Connect to ElevenLabs API, start stream |
| Speaking     | Red button    | Stream mic input                        |
| AI Responds  | Red button    | Play received voice                     |
| Button Press | Green button  | Disconnect API stream, stop mic/audio   |

---

## ğŸ§© Dependencies

- SwiftUI
- AVFoundation (for mic + playback)
- URLSession WebSockets or async HTTP stream (for 11labs)
- Combine (optional for reactive updates)

---

## ğŸ§ª Edge Cases to Handle

- Mic permission denied
- API timeout or 429 throttling
- User presses button mid-response
- No internet connection

---

## ğŸ›  Build Constraints

- SwiftUI only (no UIKit)
- No 3rd-party dependencies unless absolutely needed
- Buildable and testable in Xcode simulator (no real mic streaming there â€” fallback for testing)

---

## âœ… Definition of Done

- App builds and runs in simulator
- Button UI reacts to call state
- Mic permission flow works
- Connects and disconnects API without crash
- Sends user voice and plays back AI audio
- Handles common errors gracefully

